{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear variables\n",
    "%reset -f\n",
    "\n",
    "# libraries\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier \n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Read Data\n",
    "data = np.load('array_data/data_64.npy',allow_pickle=True)\n",
    "labels = np.load('array_data/labels.npy',allow_pickle=True)\n",
    "\n",
    "# Scale Data\n",
    "scale_data = StandardScaler().fit_transform(data)\n",
    "\n",
    "# Delete\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter Tuning\n",
    "\n",
    "# ANN\n",
    "def ann(x, y, test):\n",
    "    print('Fitting ANN...')\n",
    "    ## create model\n",
    "    ann = MLPClassifier(max_iter=1000,\n",
    "                        random_state=1)\n",
    "    ## parameters to tune\n",
    "    param_grid = {'hidden_layer_sizes':[(10,),(50,),(100,),(200,)],\n",
    "                  'activation':['tanh', 'relu'],\n",
    "                  'alpha':[0.001, 0.01, 0.1]\n",
    "                 }\n",
    "    ## use random search to tune parameters\n",
    "    ann_gscv = RandomizedSearchCV(ann, param_grid, cv=5)\n",
    "    ## fit model to data\n",
    "    ann_gscv.fit(x, y)\n",
    "    print(ann_gscv.best_params_)\n",
    "    ## predict\n",
    "    ann_pred = ann_gscv.predict(test)\n",
    "    return ann_pred\n",
    "\n",
    "# KNN\n",
    "def knn(x, y, test):\n",
    "    print('Fitting KNN...')\n",
    "    ## create model\n",
    "    knn = KNeighborsClassifier()\n",
    "    ## parameters to tune\n",
    "    param_grid = {'n_neighbors': np.arange(50, 200),\n",
    "                 'weights':['uniform','distance'],\n",
    "                 'p':[1,2]}\n",
    "    ## use random search to tune parameters\n",
    "    knn_gscv = RandomizedSearchCV(knn, param_grid, cv=5)\n",
    "    ## fit model to data\n",
    "    knn_gscv.fit(x, y)\n",
    "    print(knn_gscv.best_params_)\n",
    "    ## predict\n",
    "    knn_pred = knn_gscv.predict(test)\n",
    "    return knn_pred\n",
    "\n",
    "# SVM\n",
    "def svm(x, y, test):\n",
    "    print('Fitting SVM...')\n",
    "    ## create model\n",
    "    svm = SVC(random_state=1)\n",
    "    ## parameters to tune\n",
    "    param_grid = {'C':[0.01,0.1,1,10],\n",
    "                  'kernel':['poly','rbf'],\n",
    "                  'decision_function_shape':['ovr'],\n",
    "                  'shrinking':[True,False]\n",
    "                 }\n",
    "    # use random search to tune parameters\n",
    "    svm_gscv = RandomizedSearchCV(svm, param_grid, cv=5)\n",
    "    # fit model to data\n",
    "    svm_gscv.fit(x, y)\n",
    "    print(svm_gscv.best_params_)\n",
    "    ## predict\n",
    "    svm_pred = svm_gscv.predict(test)\n",
    "    return svm_pred\n",
    "\n",
    "# Logistic Regression\n",
    "def log_reg(x, y, test):\n",
    "    print('Fitting Logistic Regression...')\n",
    "    ## create model\n",
    "    log_reg = LogisticRegression(random_state=1)\n",
    "    ## parameters to tune\n",
    "    param_grid = {'penalty':['l1','l2','elasticnet'],\n",
    "                  'C':[0.01,0.1,1],\n",
    "                  'solver':['newton-cg', 'saga']\n",
    "                 }\n",
    "    ## use random search to tune parameters\n",
    "    log_reg_gscv = RandomizedSearchCV(log_reg, param_grid, cv=5)\n",
    "    ## fit model to data\n",
    "    log_reg_gscv.fit(x, y)\n",
    "    print(log_reg_gscv.best_params_)\n",
    "    ## predict\n",
    "    log_pred = log_reg_gscv.predict(test)\n",
    "    return log_pred\n",
    "\n",
    "# Naive Bayes\n",
    "def bayes(x, y, test):\n",
    "    print('Fitting Naive Bayes...')\n",
    "    ## create model\n",
    "    bayes = GaussianNB()\n",
    "    ## parameters to tune\n",
    "    param_grid = {'var_smoothing':[1e-9,1e-7,1e-5,1e-3,1e-1,1e0]\n",
    "                 }\n",
    "    ## use random search to tune parameters\n",
    "    bayes_gscv = RandomizedSearchCV(bayes, param_grid, cv=5)\n",
    "    ## fit model to data\n",
    "    bayes_gscv.fit(x, y)\n",
    "    print(bayes_gscv.best_params_)\n",
    "    ## predict\n",
    "    bayes_pred = bayes_gscv.predict(test)\n",
    "    return bayes_pred\n",
    "\n",
    "# Gradient Boosting\n",
    "def grad_boost(x, y, test):\n",
    "    print('Fitting Gradient Boosting...')\n",
    "    ## create model\n",
    "    grad = AdaBoostClassifier(random_state=1)\n",
    "    ## parameters to tune\n",
    "    param_grid = {'n_estimators':[100,200,300,400,500],\n",
    "                  'learning_rate':[0.01,0.1,1],\n",
    "                 }\n",
    "    ## use random search to tune parameters\n",
    "    grad_gscv = RandomizedSearchCV(grad, param_grid, cv=5)\n",
    "    ## fit model to data\n",
    "    grad_gscv.fit(x, y)\n",
    "    print(grad_gscv.best_params_)\n",
    "    ## predict\n",
    "    grad_pred = grad_gscv.predict(test)\n",
    "    return grad_pred\n",
    "\n",
    "# K-Means\n",
    "def kmeans(x,y):\n",
    "    print('Fitting K-Means...')\n",
    "    ## create model\n",
    "    kmeans = KMeans(random_state=1)\n",
    "    ## parameters to tune\n",
    "    param_grid = {'algorithm':['full','elkan'],\n",
    "                  'n_init':[10,20,30,40],\n",
    "                  'n_clusters':[2]\n",
    "                 }\n",
    "    ## use random search to tune parameters\n",
    "    kmeans_gscv = RandomizedSearchCV(kmeans, param_grid, cv=5)\n",
    "    ## fit model to data\n",
    "    kmeans_gscv.fit(x)\n",
    "    print(kmeans_gscv.best_params_)\n",
    "\n",
    "# Performance\n",
    "def perf(pred, real):\n",
    "    print('Confusion Matrix')\n",
    "    # confusion matrix\n",
    "    conf_mat = confusion_matrix(real,pred)\n",
    "    display(pd.DataFrame(conf_mat))\n",
    "    print('Classification Report')\n",
    "    # classificaiton report\n",
    "    class_rep = classification_report(real,pred,output_dict=True)\n",
    "    display(pd.DataFrame(class_rep))\n",
    "\n",
    "# Full Implementation\n",
    "def run(model,x_train,y_train,x_test,y_test):\n",
    "    # Fit and Predict\n",
    "    pred = model(x_train, y_train, x_test)\n",
    "    # Examine Performance\n",
    "    perf(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No Finding vs. Effusion, Balanced\n",
    "\n",
    "# Subset Infiltration & No Finding\n",
    "## subset\n",
    "data_sub = scale_data[:,np.where((labels=='No Finding')|(labels=='Effusion'))[0]]\n",
    "label_sub = labels[np.where((labels=='No Finding')|(labels=='Effusion'))[0]]\n",
    "## convert response to binary\n",
    "label_bin = np.where(label_sub == 'No Finding', 0, 1)\n",
    "\n",
    "# Delete\n",
    "del label_sub, scale_data, labels\n",
    "\n",
    "# Balance (Undersampliung)\n",
    "## separate\n",
    "no_finding = np.where(label_bin==0)[0]\n",
    "finding = np.where(label_bin==1)[0]\n",
    "## set seed\n",
    "np.random.seed(1)\n",
    "## under sample\n",
    "no_finding = np.random.choice(no_finding,\n",
    "                              len(finding),\n",
    "                              replace=False)\n",
    "## indicies\n",
    "ind = np.append(no_finding,finding)\n",
    "## balance\n",
    "data_small = data_sub[:,ind]\n",
    "label_small = label_bin[ind]\n",
    "\n",
    "# Delete\n",
    "del no_finding, finding, ind, data_sub, label_bin\n",
    "\n",
    "# Split Data\n",
    "(x_train,x_test,\n",
    "y_train,y_test) = train_test_split(data_small.T,label_small, \n",
    "                                       test_size=0.2, random_state=1,\n",
    "                                       shuffle=True,stratify=label_small)\n",
    "\n",
    "# Delete\n",
    "del data_small, label_small\n",
    "\n",
    "# Non-PCA Fitting (higher accuracy, slower computation)\n",
    "print('Non-PCA Data')\n",
    "# Tune Parameters\n",
    "# ANN\n",
    "run(ann,x_train,y_train,x_test,y_test)\n",
    "# KNN\n",
    "run(knn,x_train,y_train,x_test,y_test)\n",
    "# SVM\n",
    "run(svm,x_train,y_train,x_test,y_test)\n",
    "# Logistic Regression\n",
    "run(log_reg,x_train,y_train,x_test,y_test)\n",
    "# Bayes\n",
    "run(bayes,x_train,y_train,x_test,y_test)\n",
    "# Gradient Boosting\n",
    "run(grad_boost,x_train,y_train,x_test,y_test)\n",
    "# K-Means\n",
    "kmeans(x_test,y_test)\n",
    "\n",
    "# Delete\n",
    "del ann_pred,knn_pred,svm_pred,log_pred,bayes_pred,grad_pred,kmeans_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA Fitting (lower accuracy, faster computation)\n",
    "print('PCA Data')\n",
    "# PCA\n",
    "x_train_pca, x_test_pca = pca(x_train, x_test, y_train)\n",
    "\n",
    "# Delete\n",
    "del x_train, x_test\n",
    "\n",
    "# Tune Parameters\n",
    "# ANN\n",
    "run(ann,x_train_pca,y_train,x_test_pca,y_test)\n",
    "# KNN\n",
    "run(knn,x_train_pca,y_train,x_test_pca,y_test)\n",
    "# SVM\n",
    "run(svm,x_train_pca,y_train,x_test_pca,y_test)\n",
    "# Logistic Regression\n",
    "run(log_reg,x_train_pca,y_train,x_test_pca,y_test)\n",
    "# Bayes\n",
    "run(bayes,x_train_pca,y_train,x_test_pca,y_test)\n",
    "# Gradient Boosting\n",
    "run(grad_boost,x_train_pca,y_train,x_test_pca,y_test)\n",
    "# K-Means\n",
    "kmeans(x_test_pca,y_test)\n",
    "\n",
    "# Delete\n",
    "del y_train, y_test, x_train_pca, x_test_pca"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
